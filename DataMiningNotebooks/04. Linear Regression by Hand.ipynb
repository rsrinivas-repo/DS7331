{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression using Gradient Descent Algorithm - by Hand\n",
    "\n",
    "- Raghu Srinivas ( rsrinivas@smu.edu) \n",
    "\n",
    "\n",
    "In this notebook, we will construct Linear Regression algorithm using gradient descent algorithm by hand. \n",
    "\n",
    "\n",
    "Gradient Descent is an iterative method of obtaining the most optimal coefficient values for linear regression. At each iteration, we evaluate the most optimal value that aims to minimize the loss function. \n",
    "\n",
    "\n",
    "\n",
    "Let us consider, 2 independent variables x1 & x2 and the target variable y. Suppose w1 & w2 are the 2 coefficients, Linear Regression hypthothesizes that \n",
    "\n",
    "$$w1.x1 + w2.x2 = y$$\n",
    "\n",
    "Our objective is to find the values for w1 & w2 that minimize the mean square error in the outcome. \n",
    "i.e. \n",
    "\n",
    "$$Error = (1/NumObs) * ( y - (w1.x1 + w2.x2) )^2$$\n",
    "\n",
    "\n",
    "The algorithm entails the following steps\n",
    "\n",
    "1. Initialize w1 & w2 to some random values\n",
    "2. Calculate yhat for all the x1's &x2's\n",
    "3. Calculate the mean error across all observations \n",
    "4. We need to update w1 & w2 such that the error from Step 3 is further reduced. How much do we reduce by?\n",
    "        -- This is governed by the gradient descent algorithm. \n",
    "        -- We will decrease by the partial derivative of the objective function.\n",
    "$$ \\frac {d(Error)}{dw} $$\n",
    "        -- We use a learning rate parameter (lambda) to further throttle the amount of decrease at each step\n",
    "\n",
    "$$ w1 = w1 - \\lambda  \\frac {d(Error)}{dw1} $$\n",
    "\n",
    "$$ w2 = w2 - \\lambda  \\frac {d(Error)}{dw2} $$\n",
    "\n",
    "        -- In simple mathematical terms, this is defined as \n",
    "$$ w = w - (w*x -y)*x $$\n",
    "\n",
    "5. We keep repeating from Step 2 to step 4 with the updated coefficient values till we obtain an acceptable loss/error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Linear Regression - A worked Example\n",
    "\n",
    "\n",
    "\n",
    "Lets solve for w1 & w2 for the following system of equations \n",
    "\n",
    "2*w1 + 4*w2 = 14\n",
    "\n",
    "3*w1 + 5*w2 = 22\n",
    "\n",
    "6*w1 + 7*w2 = 39\n",
    "\n",
    "10*w1 + 20*w2 = 67\n",
    "\n",
    "\n",
    "We will now apply our iterative method to solve for w1 & w2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ==>    Loss : 202.10, w1=1.21 , w2=1.32\n",
      "Epoch : 1 ==>    Loss : 107.47, w1=1.42 , w2=1.61\n",
      "Epoch : 2 ==>    Loss : 47.63, w1=1.61 , w2=1.85\n",
      "Epoch : 3 ==>    Loss : 14.27, w1=1.80 , w2=2.07\n",
      "Epoch : 4 ==>    Loss : 0.91, w1=1.99 , w2=2.26\n",
      "Epoch : 5 ==>    Loss : 2.56, w1=2.16 , w2=2.43\n",
      "Epoch : 6 ==>    Loss : 15.33, w1=2.33 , w2=2.58\n",
      "Epoch : 7 ==>    Loss : 36.24, w1=2.49 , w2=2.71\n",
      "Epoch : 8 ==>    Loss : 62.99, w1=2.65 , w2=2.82\n",
      "Epoch : 9 ==>    Loss : 93.79, w1=2.80 , w2=2.92\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "\n",
    "## This will be our step-wise loss function \n",
    "def calcDelta(y,x,w):\n",
    "    delta = (w*x-y)*x\n",
    "    return delta\n",
    "\n",
    "\n",
    "## Lets intialize our X's and Y's as 1d arrays ( or python lists in our case)\n",
    "x1 = [2,3,6,10]\n",
    "x2 = [4,5,7,20]\n",
    "y = [14,22,39,67]\n",
    "\n",
    "\n",
    "\n",
    "## n denotes the number of observations. \n",
    "n = len(y)\n",
    "\n",
    "\n",
    "## We will randomly initialize w1 and w2.\n",
    "w1 =1  \n",
    "w2 =1 \n",
    "\n",
    "## We will set our learning rate here\n",
    "lr = .001\n",
    "\n",
    "## We will run our algorithm for 10 iterations \n",
    "epochs = 10\n",
    "\n",
    "for eps in range(epochs):\n",
    "\n",
    "            \n",
    "        \n",
    "    ## Now lets get into implementing Gradient Descent algorithm\n",
    "    w1_totalDelta = 0\n",
    "    w2_totalDelta = 0\n",
    "    for j in range(n):\n",
    "        ## We calculate the partial derivative for each observation (x's & y's)\n",
    "        w1_totalDelta += calcDelta(y[j],x1[j],w1)\n",
    "        w2_totalDelta += calcDelta(y[j],x2[j],w2)\n",
    "    \n",
    "    w1_avgdelta = (1/n)*w1_totalDelta  ##We calculate the average partial derivative \n",
    "    w2_avgdelta = (1/n)*w2_totalDelta\n",
    "    \n",
    "    w1 = w1 - lr*w1_avgdelta\n",
    "    w2 = w2 - lr*w2_avgdelta\n",
    "    \n",
    "    ## Lets calculate the loss for a given set of  w1 & w2\n",
    "    loss=0 \n",
    "    for i in range(n):\n",
    "        loss = math.pow((y[i] - (w1*x1[i]+w2*x2[i])),2)\n",
    "        \n",
    "    loss = loss/n   ## This is our average loss across all observations. \n",
    "\n",
    "    \n",
    "    if (eps%1==0):\n",
    "        print(\"Epoch :%2d ==>    Loss : %.2f, w1=%.2f , w2=%.2f\"%(eps,loss,w1,w2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a  Bias Term\n",
    "\n",
    "Lets try to solve for the same set of equations, but account for a bias term. Does that reduce the loss?\n",
    "\n",
    "2*w1 + 4*w2 + b = 14\n",
    "\n",
    "3*w1 + 5*w2 + b = 22\n",
    "\n",
    "6*w1 + 7*w2 + b = 39\n",
    "\n",
    "10*w1 + 20*w2 + b = 67\n",
    "\n",
    "\n",
    "We apply a simple trick to solve for the bias term. We do this by assuming X3, a new independent variable whose value is 1. \n",
    "ie\n",
    "\n",
    "2*w1 + 4*w2 + 1*b = 14\n",
    "\n",
    "3*w1 + 5*w2 + 1*b = 22\n",
    "\n",
    "6*w1 + 7*w2 + 1*b = 39\n",
    "\n",
    "10*w1 + 20*w2 + 1*b = 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ==>    Loss : 187.66, b =1.03  w1=1.21 , w2=1.32\n",
      "Epoch : 1 ==>    Loss : 96.67, b =1.07  w1=1.42 , w2=1.61\n",
      "Epoch : 2 ==>    Loss : 40.32, b =1.10  w1=1.61 , w2=1.85\n",
      "Epoch : 3 ==>    Loss : 10.29, b =1.14  w1=1.80 , w2=2.07\n",
      "Epoch : 4 ==>    Loss : 0.14, b =1.17  w1=1.99 , w2=2.26\n",
      "Epoch : 5 ==>    Loss : 4.85, b =1.21  w1=2.16 , w2=2.43\n",
      "Epoch : 6 ==>    Loss : 20.58, b =1.24  w1=2.33 , w2=2.58\n",
      "Epoch : 7 ==>    Loss : 44.33, b =1.28  w1=2.49 , w2=2.71\n",
      "Epoch : 8 ==>    Loss : 73.81, b =1.31  w1=2.65 , w2=2.82\n",
      "Epoch : 9 ==>    Loss : 107.25, b =1.34  w1=2.80 , w2=2.92\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "\n",
    "\n",
    "x0 = [1,1,1,1]   ##We define x0\n",
    "x1 = [2,3,6,10]\n",
    "x2 = [4,5,7,20]\n",
    "y = [14,22,39,67]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = len(y)\n",
    "b = 1\n",
    "w1 =1 \n",
    "w2 =1 \n",
    "lr = .001\n",
    "epochs = 10\n",
    "\n",
    "for eps in range(epochs):\n",
    "    \n",
    "        \n",
    "    w1_totalDelta = 0\n",
    "    w2_totalDelta = 0\n",
    "    b_totalDelta = 0  ## Account for Bias in our gradient calculation\n",
    "    \n",
    "    for j in range(n):\n",
    "        b_totalDelta += calcDelta(y[j],x0[j],b)\n",
    "        w1_totalDelta += calcDelta(y[j],x1[j],w1)\n",
    "        w2_totalDelta += calcDelta(y[j],x2[j],w2)\n",
    "    \n",
    "    b_avgdelta = (1/n)*b_totalDelta\n",
    "    w1_avgdelta = (1/n)*w1_totalDelta\n",
    "    w2_avgdelta = (1/n)*w2_totalDelta\n",
    "    \n",
    "    b  = b - lr*b_avgdelta\n",
    "    w1 = w1 - lr*w1_avgdelta\n",
    "    w2 = w2 - lr*w2_avgdelta\n",
    "\n",
    "    loss=0 \n",
    "    for i in range(n):\n",
    "        loss = math.pow((y[i] - (w1*x1[i]+w2*x2[i]+b*x0[i\n",
    "                                                       ])),2) ## Update the loss function to include the bias term.\n",
    "    loss = loss/n\n",
    "    \n",
    "    \n",
    "\n",
    "    if (eps%1==0):\n",
    "        print(\"Epoch :%2d ==>    Loss : %.2f, b =%.2f  w1=%.2f , w2=%.2f\"%(eps,loss,b,w1,w2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question : What impact does the bias have on the loss? Does it help reduce the loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question : Try running the above code for different values of LearningRates,Epochs & initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question : Solve for the following equation\n",
    "\n",
    " 4*w1 + 5*w2 + 3*w3 + b = 11\n",
    " \n",
    " 2*w1 + 1*w2 + 2*w3 + b = 5\n",
    " \n",
    " 7*w1 + 2*w2 + 5*w3 + b = 9\n",
    " \n",
    " 3*w1 + 3*w2 + 1*w3 + b = 8\n",
    " \n",
    " 9*w1 + 6*w2 + 6*w3 + b = 15\n",
    " \n",
    " 10*w1 + 8*w2 + 9*w3 + b = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ==>    Loss : 1051.62, w1=0.13 , w2=0.14\n",
      "Epoch : 1 ==>    Loss : 1015.28, w1=0.15 , w2=0.19\n",
      "Epoch : 2 ==>    Loss : 979.93, w1=0.18 , w2=0.23\n",
      "Epoch : 3 ==>    Loss : 945.57, w1=0.20 , w2=0.27\n",
      "Epoch : 4 ==>    Loss : 912.17, w1=0.23 , w2=0.32\n",
      "Epoch : 5 ==>    Loss : 879.71, w1=0.25 , w2=0.36\n",
      "Epoch : 6 ==>    Loss : 848.16, w1=0.28 , w2=0.40\n",
      "Epoch : 7 ==>    Loss : 817.49, w1=0.30 , w2=0.44\n",
      "Epoch : 8 ==>    Loss : 787.71, w1=0.32 , w2=0.48\n",
      "Epoch : 9 ==>    Loss : 758.77, w1=0.35 , w2=0.52\n",
      "Epoch :10 ==>    Loss : 730.66, w1=0.37 , w2=0.56\n",
      "Epoch :11 ==>    Loss : 703.36, w1=0.40 , w2=0.60\n",
      "Epoch :12 ==>    Loss : 676.85, w1=0.42 , w2=0.64\n",
      "Epoch :13 ==>    Loss : 651.11, w1=0.45 , w2=0.68\n",
      "Epoch :14 ==>    Loss : 626.13, w1=0.47 , w2=0.71\n",
      "Epoch :15 ==>    Loss : 601.88, w1=0.49 , w2=0.75\n",
      "Epoch :16 ==>    Loss : 578.35, w1=0.52 , w2=0.79\n",
      "Epoch :17 ==>    Loss : 555.52, w1=0.54 , w2=0.82\n",
      "Epoch :18 ==>    Loss : 533.38, w1=0.56 , w2=0.86\n",
      "Epoch :19 ==>    Loss : 511.90, w1=0.59 , w2=0.89\n",
      "Epoch :20 ==>    Loss : 491.07, w1=0.61 , w2=0.93\n",
      "Epoch :21 ==>    Loss : 470.89, w1=0.64 , w2=0.96\n",
      "Epoch :22 ==>    Loss : 451.32, w1=0.66 , w2=1.00\n",
      "Epoch :23 ==>    Loss : 432.36, w1=0.68 , w2=1.03\n",
      "Epoch :24 ==>    Loss : 413.99, w1=0.71 , w2=1.06\n",
      "Epoch :25 ==>    Loss : 396.19, w1=0.73 , w2=1.10\n",
      "Epoch :26 ==>    Loss : 378.96, w1=0.75 , w2=1.13\n",
      "Epoch :27 ==>    Loss : 362.28, w1=0.77 , w2=1.16\n",
      "Epoch :28 ==>    Loss : 346.14, w1=0.80 , w2=1.19\n",
      "Epoch :29 ==>    Loss : 330.52, w1=0.82 , w2=1.22\n",
      "Epoch :30 ==>    Loss : 315.41, w1=0.84 , w2=1.25\n",
      "Epoch :31 ==>    Loss : 300.80, w1=0.86 , w2=1.28\n",
      "Epoch :32 ==>    Loss : 286.68, w1=0.89 , w2=1.31\n",
      "Epoch :33 ==>    Loss : 273.03, w1=0.91 , w2=1.34\n",
      "Epoch :34 ==>    Loss : 259.84, w1=0.93 , w2=1.37\n",
      "Epoch :35 ==>    Loss : 247.11, w1=0.95 , w2=1.40\n",
      "Epoch :36 ==>    Loss : 234.82, w1=0.98 , w2=1.43\n",
      "Epoch :37 ==>    Loss : 222.96, w1=1.00 , w2=1.46\n",
      "Epoch :38 ==>    Loss : 211.51, w1=1.02 , w2=1.49\n",
      "Epoch :39 ==>    Loss : 200.48, w1=1.04 , w2=1.51\n",
      "Epoch :40 ==>    Loss : 189.85, w1=1.06 , w2=1.54\n",
      "Epoch :41 ==>    Loss : 179.60, w1=1.09 , w2=1.57\n",
      "Epoch :42 ==>    Loss : 169.74, w1=1.11 , w2=1.59\n",
      "Epoch :43 ==>    Loss : 160.25, w1=1.13 , w2=1.62\n",
      "Epoch :44 ==>    Loss : 151.12, w1=1.15 , w2=1.65\n",
      "Epoch :45 ==>    Loss : 142.34, w1=1.17 , w2=1.67\n",
      "Epoch :46 ==>    Loss : 133.91, w1=1.19 , w2=1.70\n",
      "Epoch :47 ==>    Loss : 125.81, w1=1.21 , w2=1.72\n",
      "Epoch :48 ==>    Loss : 118.04, w1=1.24 , w2=1.75\n",
      "Epoch :49 ==>    Loss : 110.59, w1=1.26 , w2=1.77\n",
      "Epoch :50 ==>    Loss : 103.45, w1=1.28 , w2=1.79\n",
      "Epoch :51 ==>    Loss : 96.61, w1=1.30 , w2=1.82\n",
      "Epoch :52 ==>    Loss : 90.07, w1=1.32 , w2=1.84\n",
      "Epoch :53 ==>    Loss : 83.82, w1=1.34 , w2=1.86\n",
      "Epoch :54 ==>    Loss : 77.85, w1=1.36 , w2=1.89\n",
      "Epoch :55 ==>    Loss : 72.16, w1=1.38 , w2=1.91\n",
      "Epoch :56 ==>    Loss : 66.73, w1=1.40 , w2=1.93\n",
      "Epoch :57 ==>    Loss : 61.56, w1=1.42 , w2=1.95\n",
      "Epoch :58 ==>    Loss : 56.65, w1=1.44 , w2=1.98\n",
      "Epoch :59 ==>    Loss : 51.99, w1=1.46 , w2=2.00\n",
      "Epoch :60 ==>    Loss : 47.57, w1=1.48 , w2=2.02\n",
      "Epoch :61 ==>    Loss : 43.38, w1=1.50 , w2=2.04\n",
      "Epoch :62 ==>    Loss : 39.43, w1=1.52 , w2=2.06\n",
      "Epoch :63 ==>    Loss : 35.69, w1=1.54 , w2=2.08\n",
      "Epoch :64 ==>    Loss : 32.18, w1=1.56 , w2=2.10\n",
      "Epoch :65 ==>    Loss : 28.88, w1=1.58 , w2=2.12\n",
      "Epoch :66 ==>    Loss : 25.78, w1=1.60 , w2=2.14\n",
      "Epoch :67 ==>    Loss : 22.89, w1=1.62 , w2=2.16\n",
      "Epoch :68 ==>    Loss : 20.20, w1=1.64 , w2=2.18\n",
      "Epoch :69 ==>    Loss : 17.69, w1=1.66 , w2=2.20\n",
      "Epoch :70 ==>    Loss : 15.37, w1=1.68 , w2=2.22\n",
      "Epoch :71 ==>    Loss : 13.24, w1=1.70 , w2=2.24\n",
      "Epoch :72 ==>    Loss : 11.28, w1=1.72 , w2=2.25\n",
      "Epoch :73 ==>    Loss : 9.50, w1=1.74 , w2=2.27\n",
      "Epoch :74 ==>    Loss : 7.88, w1=1.76 , w2=2.29\n",
      "Epoch :75 ==>    Loss : 6.43, w1=1.78 , w2=2.31\n",
      "Epoch :76 ==>    Loss : 5.13, w1=1.80 , w2=2.33\n",
      "Epoch :77 ==>    Loss : 3.99, w1=1.82 , w2=2.34\n",
      "Epoch :78 ==>    Loss : 3.01, w1=1.83 , w2=2.36\n",
      "Epoch :79 ==>    Loss : 2.17, w1=1.85 , w2=2.38\n",
      "Epoch :80 ==>    Loss : 1.47, w1=1.87 , w2=2.39\n",
      "Epoch :81 ==>    Loss : 0.91, w1=1.89 , w2=2.41\n",
      "Epoch :82 ==>    Loss : 0.49, w1=1.91 , w2=2.43\n",
      "Epoch :83 ==>    Loss : 0.20, w1=1.93 , w2=2.44\n",
      "Epoch :84 ==>    Loss : 0.04, w1=1.95 , w2=2.46\n",
      "Epoch :85 ==>    Loss : 0.00, w1=1.96 , w2=2.47\n",
      "Epoch :86 ==>    Loss : 0.09, w1=1.98 , w2=2.49\n",
      "Epoch :87 ==>    Loss : 0.29, w1=2.00 , w2=2.50\n",
      "Epoch :88 ==>    Loss : 0.61, w1=2.02 , w2=2.52\n",
      "Epoch :89 ==>    Loss : 1.04, w1=2.04 , w2=2.53\n",
      "Epoch :90 ==>    Loss : 1.59, w1=2.06 , w2=2.55\n",
      "Epoch :91 ==>    Loss : 2.23, w1=2.07 , w2=2.56\n",
      "Epoch :92 ==>    Loss : 2.98, w1=2.09 , w2=2.58\n",
      "Epoch :93 ==>    Loss : 3.83, w1=2.11 , w2=2.59\n",
      "Epoch :94 ==>    Loss : 4.78, w1=2.13 , w2=2.60\n",
      "Epoch :95 ==>    Loss : 5.82, w1=2.15 , w2=2.62\n",
      "Epoch :96 ==>    Loss : 6.96, w1=2.16 , w2=2.63\n",
      "Epoch :97 ==>    Loss : 8.18, w1=2.18 , w2=2.65\n",
      "Epoch :98 ==>    Loss : 9.49, w1=2.20 , w2=2.66\n",
      "Epoch :99 ==>    Loss : 10.89, w1=2.22 , w2=2.67\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "\n",
    "## This will be our step-wise loss function \n",
    "def calcDelta(y,x,w):\n",
    "    delta = (w*x-y)*x\n",
    "    return delta\n",
    "\n",
    "\n",
    "## Lets intialize our X's and Y's as 1d arrays ( or python lists in our case)\n",
    "x1 = [2,3,6,10]\n",
    "x2 = [4,5,7,20]\n",
    "y = [14,21,41,69]\n",
    "\n",
    "\n",
    "\n",
    "## n denotes the number of observations. \n",
    "n = len(y)\n",
    "\n",
    "\n",
    "## We will randomly initialize w1 and w2.\n",
    "w1 =  .1\n",
    "w2 = .1\n",
    "\n",
    "## We will set our learning rate here\n",
    "lr = .0001\n",
    "\n",
    "## We will run our algorithm for 10 iterations \n",
    "epochs = 100\n",
    "\n",
    "for eps in range(epochs):\n",
    "\n",
    "            \n",
    "        \n",
    "    ## Now lets get into implementing Gradient Descent algorithm\n",
    "    w1_totalDelta = 0\n",
    "    w2_totalDelta = 0\n",
    "    for j in range(n):\n",
    "        ## We calculate the partial derivative for each observation (x's & y's)\n",
    "        w1_totalDelta += calcDelta(y[j],x1[j],w1)\n",
    "        w2_totalDelta += calcDelta(y[j],x2[j],w2)\n",
    "    \n",
    "    w1_avgdelta = (1/n)*w1_totalDelta  ##We calculate the average partial derivative \n",
    "    w2_avgdelta = (1/n)*w2_totalDelta\n",
    "    \n",
    "    w1 = w1 - lr*w1_avgdelta \n",
    "    w2 = w2 - lr*w2_avgdelta \n",
    "    \n",
    "    ## Lets calculate the loss for a given set of  w1 & w2\n",
    "    loss=0 \n",
    "    for i in range(n):\n",
    "        loss = math.pow((y[i] - (w1*x1[i]+w2*x2[i])),2)\n",
    "        \n",
    "    loss = loss/n   ## This is our average loss across all observations. \n",
    "\n",
    "    \n",
    "    if (eps%1==0):\n",
    "        print(\"Epoch :%2d ==>    Loss : %.2f, w1=%.2f , w2=%.2f\"%(eps,loss,w1,w2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
