{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression using Gradient Descent Algorithm - by Hand\n",
    "\n",
    "- Raghu Srinivas ( rsrinivas@smu.edu) \n",
    "\n",
    "\n",
    "In this notebook, we will construct Logistic Regression algorithm using gradient descent algorithm by hand. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression - A worked Example\n",
    "\n",
    "\n",
    "In this example, we load a dataset that contains the 2 independent variables \n",
    "years : indicating the number of years a1c of a patient has been measured\n",
    "a1c : the latest a1c scores of the patient\n",
    "\n",
    "The target variable y indicates if the patient is diabetic or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   years  a1c  y\n",
      "0      3    7  1\n",
      "1      2    3  0\n",
      "2      3    7  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"data/logisticregression_1.csv\")\n",
    "print(data.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ==>    Loss : 4.30, w1=0.50 , w2=0.50\n",
      "Epoch :300 ==>    Loss : 1.27, w1=0.24 , w2=0.35\n",
      "Epoch :600 ==>    Loss : 0.40, w1=0.12 , w2=0.25\n",
      "Epoch :900 ==>    Loss : 0.14, w1=0.06 , w2=0.18\n",
      "Epoch :1200 ==>    Loss : 0.05, w1=0.03 , w2=0.13\n",
      "Epoch :1500 ==>    Loss : 0.02, w1=0.01 , w2=0.09\n",
      "Epoch :1800 ==>    Loss : 0.01, w1=0.01 , w2=0.06\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import numpy as np  \n",
    "\n",
    "## This will be our step-wise loss function \n",
    "def calcDelta(y,x,w):\n",
    "    delta = x*w*(1-y)\n",
    "    return delta\n",
    "\n",
    "\n",
    "\n",
    "## Lets intialize our X's and Y's as 1d arrays ( or python lists in our case)\n",
    "x1 = list(data[data.columns[0]].values)\n",
    "x2 = list(data[data.columns[1]].values)\n",
    "y = list(data.y.values)\n",
    "\n",
    "\n",
    "\n",
    "## n denotes the number of observations. \n",
    "n = len(y)\n",
    "\n",
    "\n",
    "## We will randomly initialize w1 and w2.\n",
    "w1 = .5 #np.random.rand()\n",
    "w2 = .5 #np.random.rand()\n",
    "\n",
    "## We will set our learning rate here\n",
    "lr = .001\n",
    "\n",
    "## We will run our algorithm for 10 iterations \n",
    "epochs = 2000\n",
    "\n",
    "for eps in range(epochs):\n",
    "\n",
    "            \n",
    "        \n",
    "    ## Now lets get into implementing Gradient Descent algorithm\n",
    "    w1_totalDelta = 0\n",
    "    w2_totalDelta = 0\n",
    "    for j in range(n):\n",
    "        ## We calculate the partial derivative for each observation (x's & y's)\n",
    "        w1_totalDelta += calcDelta(y[j],x1[j],w1)\n",
    "        w2_totalDelta += calcDelta(y[j],x2[j],w2)\n",
    "    \n",
    "    w1_avgdelta = (1/n)*w1_totalDelta  ##We calculate the average partial derivative \n",
    "    w2_avgdelta = (1/n)*w2_totalDelta\n",
    "    \n",
    "    w1 = w1 - lr*w1_avgdelta\n",
    "    w2 = w2 - lr*w2_avgdelta\n",
    "    \n",
    "    ## Lets calculate the loss for a given set of  w1 & w2\n",
    "    loss=0 \n",
    "    for i in range(n):\n",
    "        loss = math.pow((y[i] - (w1*x1[i]+w2*x2[i])),2)\n",
    "        \n",
    "    loss = loss/n   ## This is our average loss across all observations. \n",
    "\n",
    "    \n",
    "    if (eps%300==0):\n",
    "        print(\"Epoch :%2d ==>    Loss : %.2f, w1=%.2f , w2=%.2f\"%(eps,loss,w1,w2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Inference\n",
    "\n",
    "\n",
    "We obtain the best estimates of w1 and w2 from the models output and use them to compute the predicted probabilities for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ytrue = 1  ypredProd = 0.61  \n",
      "ytrue = 0  ypredProd = 0.55  \n",
      "ytrue = 1  ypredProd = 0.61  \n",
      "ytrue = 1  ypredProd = 0.63  \n",
      "ytrue = 1  ypredProd = 0.62  \n",
      "ytrue = 0  ypredProd = 0.55  \n",
      "ytrue = 0  ypredProd = 0.56  \n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "w1 = .01\n",
    "w2 = .06\n",
    "for i in range(n):\n",
    "    yhat = w1*x1[i] + w2*x2[i]\n",
    "    print(\"ytrue = %d  ypredProd = %.2f  \"%(y[i],sigmoid(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classification Threshold\n",
    "\n",
    "\n",
    "For this toy example, we can pick a classification threshold of .60 to obtain a 100% model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inclass Question \n",
    "\n",
    "\n",
    "Rerun the model with a bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
